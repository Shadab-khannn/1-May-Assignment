{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2492a-373b-42d0-aa15-6de4c1105988",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013dda89-e59c-438e-8428-0cc82c969c49",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66aff4-f068-46ba-9712-102f51b04250",
   "metadata": {},
   "outputs": [],
   "source": [
    "A contingency matrix is a table that shows the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives \n",
    "(FN) for a classification model. \n",
    "The rows of the table represent the actual classes, and the columns represent the predicted classes.\n",
    "\n",
    "The contingency matrix is used to evaluate the performance of a classification model by calculating various metrics such as accuracy,\n",
    "precision, recall, and F1 score.\n",
    "These metrics can be calculated from the values in the contingency matrix as follows:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "The contingency matrix can also be used to visualize the performance of a classification model by showing how well the model is able to \n",
    "correctly classify samples into different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eab74b-a1d4-4fc8-9d44-eeb7e8e52a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afe127-2fb7-45f6-b478-f34ead771061",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "certain situations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8a222-dc12-4830-a855-8afb58bc9e1b",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425a935-9bd4-41cf-9f60-cfbeb6850748",
   "metadata": {},
   "outputs": [],
   "source": [
    "A pair confusion matrix is a table that shows the number of times each pair of classes is confused by a classification model.\n",
    "The rows of the table represent the actual classes, and the columns represent the predicted classes.\n",
    "\n",
    "A pair confusion matrix is different from a regular confusion matrix because it provides more detailed information about how a classification\n",
    "model is performing for each pair of classes. This can be useful in certain situations where it is important to know which pairs of classes\n",
    "are being confused by the model.\n",
    "\n",
    "For example, in a medical diagnosis problem, it might be more important to know which pairs of diseases are being confused by the model than\n",
    "to know the overall accuracy of the model.\n",
    "This information can help doctors to identify which diseases are most likely to be misdiagnosed and take appropriate action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4439b96-926b-418b-a9f7-40ed3c63967a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7ed1e-ac01-481f-bc8a-6c4d033e2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "used to evaluate the performance of language models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd0c8d-b236-4c43-8acb-4b033830ffa4",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ae15d-ee32-4f65-8240-c4461c5c3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "In natural language processing, an extrinsic measure is a way of evaluating the performance of a language model by measuring its performance\n",
    "on a specific task that it was designed to perform.\n",
    "Extrinsic measures are used to evaluate the effectiveness of a language model in real-world applications.\n",
    "\n",
    "For example, if a language model is designed to perform sentiment analysis on social media posts, an extrinsic measure would evaluate the \n",
    "model’s performance on this specific task.\n",
    "The accuracy of the model on this task would be used as an extrinsic measure of its performance.\n",
    "\n",
    "Extrinsic measures are often used in combination with intrinsic measures, which evaluate the quality of a language model independent of\n",
    "any specific task. \n",
    "Intrinsic measures are useful for evaluating the quality of a language model in general, while extrinsic measures are useful for evaluating\n",
    "its performance in specific applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4aa403-0ce4-459d-a6be-471aa33559a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b6f7f-dc67-4bf3-bbe2-35a48448811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "extrinsic measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060250c-1a58-4455-b6a1-c0e88dbb9c6f",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df22722-10c1-4a47-abe4-0e47f06a1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "In machine learning, an intrinsic measure is a way of evaluating the quality of a model independent of any specific task. \n",
    "Intrinsic measures are used to evaluate the effectiveness of a model in general.\n",
    "\n",
    "For example, if a model is designed to perform image classification, an intrinsic measure would evaluate the quality of the model’s feature \n",
    "representation. \n",
    "The accuracy of the model on this task would be used as an extrinsic measure of its performance.\n",
    "\n",
    "Intrinsic measures are often used in combination with extrinsic measures, which evaluate the performance of a model on specific tasks. \n",
    "Extrinsic measures are useful for evaluating the effectiveness of a model in real-world applications, while intrinsic measures are useful \n",
    "for evaluating its quality in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9beceb-c3a8-4f5f-b2d9-182600d490f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f0688-10b3-4a0f-929f-49f06f9dd0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "strengths and weaknesses of a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22ce86-2d6c-4e35-b282-1df811dd29de",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feedb77-0acc-4af6-b811-2bd7e7d702cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A confusion matrix is a table that is used to evaluate the performance of a classification model. \n",
    "The table shows the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) for each class in \n",
    "the classification model.\n",
    "\n",
    "The purpose of a confusion matrix is to help identify the strengths and weaknesses of a model. By analyzing the values in the confusion matrix,\n",
    "it is possible to determine which classes are being classified correctly and which classes are being misclassified. \n",
    "This information can be used to improve the model by adjusting the parameters or features used in the model.\n",
    "\n",
    "For example, if a model is misclassifying a particular class, it may be necessary to collect more data for that class or adjust the \n",
    "features used in the model to better distinguish between that class and other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99fdbe-d8d8-4e6b-b880-35e57acecc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e84472-048a-4546-89d9-075081e1a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "learning algorithms, and how can they be interpreted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e993f00-b2af-4ec5-9135-52888758f3d2",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20c5bf-d548-4a65-8e30-2692e94c3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Common intrinsic measures used to evaluate the performance of unsupervised learning algorithms include:\n",
    "\n",
    "1.Silhouette score: This measure evaluates the quality of clustering by measuring how similar an object is to its own cluster compared to\n",
    "                    other clusters. The score ranges from -1 to 1, with higher scores indicating better clustering.\n",
    "\n",
    "2.Calinski-Harabasz index: This measure evaluates the quality of clustering by measuring the ratio of between-cluster variance to \n",
    "                           within-cluster variance. Higher values indicate better clustering.\n",
    "\n",
    "3.Davies-Bouldin index: This measure evaluates the quality of clustering by measuring the similarity between clusters.\n",
    "                        Lower values indicate better clustering.\n",
    "\n",
    "These measures can be interpreted as follows:\n",
    "\n",
    "**Silhouette score: A score close to 1 indicates that an object is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
    "                    A score close to -1 indicates that an object is poorly matched to its own cluster and well-matched to neighboring clusters.\n",
    "\n",
    "**Calinski-Harabasz index: A higher value indicates that the clusters are more separated and distinct.\n",
    "\n",
    "**Davies-Bouldin index: A lower value indicates that the clusters are more separated and distinct.\n",
    "\n",
    "These measures can be used to evaluate the quality of unsupervised learning algorithms and identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455893ed-10e8-4f60-8526-d65a33192a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3a249-2716-4154-a2fc-023a6ae5e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "how can these limitations be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946dfc94-992f-4e56-b34c-abb8f4f3d7ae",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe986f-08fd-4551-afa3-10a5e667a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy is a commonly used evaluation metric for classification tasks. However, there are some limitations to using accuracy as a sole \n",
    "evaluation metric:\n",
    "\n",
    "1. Class imbalance: If the data is not balanced, accuracy can be biased towards classes with a higher number of counts. \n",
    "For example, if the data contains only 10% of positive instances, a majority baseline classifier which always assigns the negative label \n",
    "would reach 90% accuracy since it would correctly predict 90% instances.\n",
    "\n",
    "2. Misclassification costs: In some cases, misclassifying one class may be more costly than misclassifying another class. \n",
    "                            In such cases, accuracy may not be an appropriate evaluation metric.\n",
    "\n",
    "3. Overfitting: If the model is overfitting the training data, it may perform well on the training data but poorly on new data.\n",
    "\n",
    "These limitations can be addressed by using other evaluation metrics in addition to accuracy. For example, precision and recall can be used \n",
    "to evaluate the performance of a classifier on each class separately.\n",
    "F1 score is another commonly used metric that takes into account both precision and recall.\n",
    "Other metrics such as ROC curve and AUC can also be used to evaluate the performance of a classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
